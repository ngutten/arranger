This is the standalone version of the webapp post refactor, and is the primary version with continued development. 

Aside from the Python libraries in requirements.txt, this requires system installs (or DLLs) for fluidsynth, ffmpeg, libfluidsynth3, and portaudio19-dev.

== Notes on standalone version refactor ==

This was interesting but a very different experience than the initial development of the web version. I had Opus do the first attempt to port the GUI over to tkinter in one shot, which worked surprisingly well but had some awkward bits (template.html was >30k tokens, so it couldn't read it all at once). That consumed a full session and about $6 of API credits on top of that, and resulted in something where you could only see one piano roll note at a time, the beat map grid was scrunched up, you couldn't drag and drop patterns onto the timeline, and the interface flickered horribly every time you did anything. Rather than try to fix that in place, I spent part of another session pivoting it over to PySide6 in pieces - I gave it a couple UI files and had it port them, gave it a couple more and had it port them, etc (Sonnet rather than Opus). This mostly worked with some awkwardness where it turned out it was really important to know something in app.py to get the UI stuff to link up correctly, but this did more or less manage the context size and prevent too much token wastage.

Following that I ended up in a bit of debug hell trying to fix a few outstanding issues. One thing I've found with these tools is that there's a very strong 90/10 principle, or maybe its more like 99/1 here - you can spend more resources fixing the last little bits than writing the entire thing from scratch. Part of this is context rot when contexts grow through repeated debug cycles (and possibly due to the negative valence of the conversation as a whole?). One warning sign is that when it starts making errors in matching up parentheses or curly braces, you should start a fresh context. But another part is the eager 'I know what's wrong!' type of guessing that generate a lot of short and useless iterations that end up expanding the code and context. Here I found that asking 'what can I do to help you debug this?' seems to bias the conversation towards 'find out what's wrong' rather than 'tell me what's wrong' and escapes some of the over-eager guessing behavior. I found this out after five or six back and forths over 'no, the song isn't actually stopping playback when its over and there's no indicator of the play position'. Once I asked to be included in the debugging, it led to a bunch of checks being placed in the code which diagnosed the error more precisely. Of course people talk about writing lots of tests with this sort of development which is the same spirit, but its just whether you anticipate the potential bug in advance versus having to diagnose a bug after the fact.

Another thing that can be useful is to just say 'rewrite the code using a different approach' - for a human developer that's painfully expensive, for an LLM it's sometimes cheaper and easier than debugging what you currently have. I had to do this for the drag and drop behavior of the patterns. After many iterations claiming 'yes, it should drag and drop now' including with the 'let me help you debug it' approach, there was very little change. Asking 'just convert it to click-to-add' solved it in one pass.

The advantage of going to the standalone version, aside from convenience and security, is realtime synthesis. For that I planned to use Opus to plan and Sonnet to implement, much like with the beatmap feature. The first attempt to do so got stuck on nasty static-filled playback and buffer overruns that the debugging methods I had previously tried failed at resolving, so I started over and tried using Opus for the implementation as well. This did work, though it cost about a session and $4 in API charges in excess beyond that. A lot of the problems in the original Sonnet attempt just did not manifest this time (though I did note them in advance to Opus), and the workflow had more in the way of explicit testing of audio engine features though for the most part those tests passed immediately on being written. 
